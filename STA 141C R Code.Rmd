---
title: "STA 141C R Code"
author: "Jenny Xu"
date: "2025-05-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggplot2)
library(pheatmap)
library(caret)
library(glmnet)
library(e1071)
library(rpart)
library(rpart.plot)
library(randomForest)
library(class)
library(MLmetrics)
library(yardstick)
library(dplyr)
library(pROC)
```

```{r}
# Exploratory Data Analysis
# Read data
data = read.csv("Breast_GSE45827.csv")

# View shape and basic structure of the dataset
dim(data)
str(data)
head(data[,1:10])

# Check the unique values and levels of outcome variable
table(data$type)
unique(data$type)

# Check missing values -- there's no missing values in our dataset
sum(is.na(data))

# Barplot of subtype distribution -- we have class imbalance
ggplot(data, aes(x = type))+
  geom_bar(fill = "steelblue")+
  labs(title = "Distribution of Breast Cancer Subtypes", x = "Cancer Subtypes", y = "Count")

# Summary statistics of 10 gene expression values
gene.columns = data[,-(1:2)]
set.seed(141)
random.genes = sample(names(gene.columns), 10)
summary(data[,random.genes])

# Distribution of mean across all genes -- we need normalization
gene.means = apply(gene.columns, 2, mean)
hist(gene.means, breaks = 100,
     main = "Distribution of Gene Means",
     xlab = "Mean Expression",
     col = "lightcoral")

# Distribution of variance across all genes -- 
# genes that differ the most across patients, predictive patterns?
gene.variances = apply(gene.columns, 2, var)
hist(gene.variances, breaks = 50,
     main = "Distribution of Gene Variances",
     xlab = "Variance",
     col = "skyblue")

top.genes = sort(gene.variances, decreasing = TRUE)
top.10.genes.name = names(top.genes)[1:10]

for (gene in top.10.genes.name){
  # New data frame
  data.plot = data.frame(
    expression = gene.columns[[gene]],
    subtype = data$type
  )
  
  # Boxplot of how gene expression differ by subtype for highly variable gene
  print(
    ggplot(data.plot, aes(x = subtype, y = expression, fill = subtype)) +
      geom_boxplot() +
      labs(title = paste("Expression of", gene, "by Subtype"),
           x = "Subtype",
           y = "Expression Level")
  )
  # One-way ANOVA for one top gene
  print(summary(aov(expression~subtype, data = data.plot)))
}

# PCA plot -- samples cluster by cancer subtype
# Standardize all genes (mean 0, sd 1)
scaled.data = scale(gene.columns)
# PCA
pca.result = prcomp(scaled.data)
pca.plot = data.frame(
  PC1 = pca.result$x[,1],
  PC2 = pca.result$x[,2],
  Subtype = data$type
)
ggplot(pca.plot, aes(x = PC1, y = PC2, color = Subtype)) +
  geom_point(size = 2) +
  labs(title = "PCA of Gene Expression (Scaled)", x = "PC1", y = "PC2") 

# Heatmap of 10 top variable genes
top.10.expression = gene.columns[,top.10.genes.name]
top.10.scaled = scale(top.10.expression)
rownames(top.10.scaled) = rownames(data)
row.annotation = data.frame(Subtype = data$type)
rownames(row.annotation) = rownames(top.10.scaled)

pheatmap(top.10.scaled,
         annotation_row = row.annotation,
         cluster_rows = TRUE,
         cluster_cols = TRUE,
         main = "Heatmap of Top 10 Variable Genes")

# Count per class
type_counts <- data %>%
  count(type) %>%
  mutate(Percentage = n / sum(n) * 100)

# Bar plot of class counts
bar_plot <- ggplot(type_counts, aes(x = type, y = n, fill = type)) +
  geom_bar(stat = "identity") +
  labs(title = "Sample Count by Class", x = "Cancer Subtype", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")

# Pie chart of class proportions
pie_plot <- ggplot(type_counts, aes(x = "", y = Percentage, fill = type)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y") +
  labs(title = "Class Distribution (Pie Chart)") +
  theme_void() +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")),
            position = position_stack(vjust = 0.5), color = "white", size = 4)

# Combine the two plots using gridExtra
library(gridExtra)

```

```{r}
# Methodology (Pre-processing steps)
# Step 1: Split into Training and Test sets
set.seed(141)
train.idx = sample(1:nrow(data), size = 0.8*nrow(data))
train.data = data[train.idx,]
test.data = data[-train.idx,]

# Step 2: Extract gene expression matrix and subtype labels
x.train.raw = train.data[,-(1:2)]
y.train = train.data$type
x.test.raw = test.data[,-(1:2)]
y.test = test.data$type

# Step 3: Scale training data only and test data using training parameters
scaled.train = scale(x.train.raw)
train.center = attr(scaled.train, "scaled:center")
train.scale = attr(scaled.train, "scaled:scale")
scaled.test = scale(x.test.raw,
                    center = train.center,
                    scale = train.scale)

# Step 4: Handle class imbalance by upsampling scaled training data
y.train = factor(train.data$type)

train.upsampled = upSample(
  x = as.data.frame(scaled.train),
  y = y.train,
  yname = "target"
)

x.train.resampled = train.upsampled[, -ncol(train.upsampled)]
y.train.resampled = train.upsampled$target

table(y.train.resampled)

```

```{r}
# Methodology (PCA)
# Step 1: Apply PCA to scaled training data
pca.model = prcomp(x.train.resampled, center = FALSE, scale. = FALSE)

# Step 2: Examine variance explained by each component
explained.variance = pca.model$sdev^2
proportion.variance = explained.variance/sum(explained.variance)
cumulative.variance = cumsum(proportion.variance)

# Step 3: Find number of PCs that explain more than 95% of variance
num.pc.95 = which(cumulative.variance >= 0.95)[1]

# Step 4: Create PCA-transformed training data
pca.train = as.data.frame(pca.model$x[,1:num.pc.95])
pca.train$Subtype = y.train.resampled

# Step 5: Apply same PCA to test data
pca.test.raw = predict(pca.model, newdata = scaled.test)
pca.test = as.data.frame(pca.test.raw[,1:num.pc.95])

# Step 6: Scree plot: proportion of variance
plot(explained.variance[1:50], type = "b",
     xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     main = "PCA Scree Plot (Top 50 PCs)")

# Step 7: Cumulative variance plot
scree.data = data.frame(
  PC = 1:length(cumulative.variance),
  CumulativeVariance = cumulative.variance
)

ggplot(scree.data, aes(x = PC, y = CumulativeVariance)) +
  geom_line(color = "blue") +
  geom_point() +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") +
  geom_vline(xintercept = num.pc.95, linetype = "dotted", color = "darkgreen") +
  theme_minimal() +
  labs(
    title = "Cumulative Variance Explained by PCA",
    x = "Number of Principal Components",
    y = "Cumulative Proportion of Variance Explained"
  )
```

```{r}
# Methodology (Lasso)
# Step 1: Prepare training and test data in matrix form
x.lasso.train = as.matrix(pca.train[,-ncol(pca.train)])
y.lasso.train = pca.train$Subtype

x.lasso.test = as.matrix(pca.test)

# Step 2: Fit Lasso Model
lasso.model = glmnet(x.lasso.train, y.lasso.train, 
                     alpha = 1, family = "multinomial")

# Step 3: Use Cross-validation to find optimal lambda
set.seed(141)
cv.lasso = cv.glmnet(x.lasso.train, y.lasso.train, 
                     alpha = 1,family = "multinomial")
plot(cv.lasso)
best.lambda = cv.lasso$lambda.min

# Step 4: Predict on test data
lasso.pred = predict(cv.lasso, newx = x.lasso.test, 
                    s = best.lambda, type = "class")

# Step 5: Evaluate Model Performance
table(Predicted = lasso.pred, Actual = test.data$type)
lasso.accruacy = mean(lasso.pred == test.data$type)
lasso.accruacy
```

```{r}
# Which gene is most predictive for each breast cancer subtype using Lasso?
# No scaling of raw data, just upsampling to avoid bias
# Step 1: Upsample raw data
set.seed(141)
train.raw.upsampled = upSample(
  x = x.train.raw,
  y = factor(y.train),
  yname = "target"
)

# Step 2: Prepare training and test data for Lasso
x.raw.lasso.train = as.matrix(train.raw.upsampled[, -ncol(train.raw.upsampled)])
y.raw.lasso.train = train.raw.upsampled$target

# Step 3: Fit lasso on raw gene data
lasso.raw.model = glmnet(x.raw.lasso.train, y.raw.lasso.train,
                         alpha = 1, family = "multinomial")

# Step 4: Cross-validation for lambda
cv.raw.lasso = cv.glmnet(x.raw.lasso.train, y.raw.lasso.train,
                         alpha = 1, family = "multinomial")
best.raw.lambda = cv.raw.lasso$lambda.min

# Step 5: Extract coefficients at best lambda
coef.raw = coef(cv.raw.lasso, s = best.raw.lambda)

# Step 6: Find top predictive genes per subtype
top.genes.by.subtype = list()

for (subtype in names(coef.raw)){
  coef.matrix = coef.raw[[subtype]]
  nonzero.idx = which(coef.matrix !=0)[-1]
  
  if (length(nonzero.idx) >0){
    gene.name = rownames(coef.matrix)[nonzero.idx]
    abs.coef = abs(as.vector(coef.matrix[nonzero.idx]))
    names(abs.coef) = gene.name
    
    top.genes = sort(abs.coef, decreasing = TRUE)
    top.genes.by.subtype[[subtype]] = top.genes
  }else{
    top.genes.by.subtype[[subtype]] = NULL
    }
}

top.genes.by.subtype
```

```{r}
# Methodology (SVM)
# Step 1: Prepare training and test data from PCA transformed data
x.svm.train = as.matrix(pca.train[, -ncol(pca.train)])
y.svm.train = pca.train$Subtype

x.svm.test = as.matrix(pca.test)

# Step 2: Train SVM Model
set.seed(141)
svm.model = svm(
  x = x.svm.train,
  y = y.svm.train,
  kernel = "linear",
  probability = TRUE
)

# Step 3: Predict on Test data
svm.pred = predict(svm.model, newdata = x.svm.test)

# Step 4: Evaluate Accruacy
table(Predicted = svm.pred, Actual = test.data$type)
svm.accruacy = mean(svm.pred == test.data$type)
svm.accruacy
```

```{r}
# Methodology (Decision Tree)
# Step 1: Prepare training and test data from PCA transformed data
tree.train.data = pca.train
tree.test.data = pca.test

# Step 3: Fit tree model
tree.model = rpart(Subtype~., data = tree.train.data, method = "class")

# Step 4: Predict on test data
tree.pred = predict(tree.model, newdata = tree.test.data, type = "class")

# Step 5: Confusion matrix + accuracy
table(Predicted = tree.pred, Actual = test.data$type)
tree.accruacy = mean(tree.pred == test.data$type)
tree.accruacy

# Step 6: Visualize the tree
rpart.plot(tree.model, main = "Decision Tree on PCA Data")
```

```{r}
# Methodology (Random Forest)
# Step 1: Prepare training and test data from PCA transformed data
rf.train.data = pca.train
rf.test.data = pca.test

# Step 3: Fit random forest model
set.seed(123)
rf.model = randomForest(Subtype~., data = rf.train.data, ntree = 500)

# Step 4: Predict on Test data
rf.pred = predict(rf.model, newdata = rf.test.data)

# Step 5: Confusion matrix + accuracy
table(Predicted = rf.pred, Actual = test.data$type)
rf.accuracy = mean(rf.pred == test.data$type)
rf.accuracy
```
```{r}
# Methodology (KNN)
# Step 1: Prepare training and test data from PCA transformed data
x.knn.train = as.matrix(pca.train[, -ncol(pca.train)])
y.knn.train = pca.train$Subtype

x.knn.test = as.matrix(pca.test)

# Step 2: fit KNN model (manually choose k)
set.seed(141)
knn.pred = knn(train = x.knn.train, test = x.knn.test,
               cl = y.knn.train, k = 5)

# Step 3: Confusion matrix + accuracy
table(Predicted = knn.pred, Actual = test.data$type)
knn.accuracy = mean(knn.pred == test.data$type)
knn.accuracy
```
```{r}
# Main Results (Model Comparison using Caret pacakage)
# Step 1: Extract features and target from PCA data
x.pca = pca.train[, -ncol(pca.train)]
y.pca = pca.train$Subtype

# Step 2: Define 5-fold CV
ctrl = trainControl(method = "cv", number = 5)

# Step 3: Train models using PCA data
set.seed(141)
model.results.pca = list()

model.results.pca[["Logistic Regression"]] = train(
  x = x.pca,y =y.pca,
  method = "multinom",
  trControl = ctrl,
  trace = FALSE
)

model.results.pca[["Lasso"]] = train(
  x = x.pca, y = y.pca,
  method = "glmnet",
  trControl = ctrl,
  tuneLength = 10
)

model.results.pca[["SVM"]] = train(
  x = x.pca, y =y.pca,
  method = "svmLinear",
  trControl = ctrl
)

model.results.pca[["Decision Tree"]] = train(
  x = x.pca,y =y.pca,
  method = "rpart",
  trControl = ctrl
)

model.results.pca[["Random Forest"]] = train(
  x = x.pca,y =y.pca,
  method = "rf",
  trControl = ctrl
)

model.results.pca[["KNN"]] = train(
  x = x.pca,y =y.pca,
  method = "knn",
  trControl = ctrl,
  tuneLength = 5
)

# Step 4: Compare PCA-based model accuracy
model.accuracies.pca = sapply(model.results.pca, 
                              function(m) max(m$results$Accuracy))
model.accuracies.pca

# Step 5: Plot
# Extract cross-validated accuracy from each caret model
model.accuracies.pca <- sapply(model.results.pca, function(m) {
  if ("Accuracy" %in% colnames(m$results)) {
    max(m$results$Accuracy)
  } else {
    NA  # Handle models like SVM (if probs or tuning failed)
  }
})

# Create data frame
accuracy_df <- data.frame(
  Model = names(model.accuracies.pca),
  Accuracy = as.numeric(model.accuracies.pca)
)

ggplot(accuracy_df, aes(x = reorder(Model, -Accuracy), y = Accuracy)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = round(Accuracy, 3)), vjust = -0.5, size = 4) +
  ylim(0, 1) +
  theme_minimal() +
  labs(
    title = "Model Accuracy on PCA-Transformed Data",
    x = "Model",
    y = "Cross-Validated Accuracy"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
# Main Results (Model Evaluation: Random Forest)
# Step 1: Store predicted labels and true test labels
true.label = factor(test.data$type)
rf.prediction = predict(model.results.pca[["Random Forest"]], pca.test)

# Step 2: Create dataframe for evaluation
evaluate.data = data.frame(
  truth = true.label,
  prediction = factor(rf.prediction, levels = levels(true.label))
)

# Step 3: F1
f1.macro = f_meas(evaluate.data, truth = truth, 
                  estimate = prediction, beta = 1)

# Step 4: F2
f2.macro = f_meas(evaluate.data, truth = truth, 
                  estimate = prediction, beta = 2)

# Step 5: Probabilities for AUC
rf.probs = predict(model.results.pca[["Random Forest"]], pca.test, type = "prob")
roc.multiclass = multiclass.roc(response = true.label, 
                                predictor = as.matrix(rf.probs))
auc.value = auc(roc.multiclass)

# Step 6: Output
f1.macro
f2.macro
auc.value

# Step 7: ROC Curve Plot
class_levels = colnames(rf.probs)

roc_df <- do.call(rbind, lapply(class_levels, function(class) {
  binary_response <- as.numeric(true.label == class)
    # Only compute ROC if both classes are present
  if (length(unique(binary_response)) < 2) {
    return(NULL)  # Skip this class
  }
  
  roc_obj <- roc(binary_response, rf.probs[[class]], quiet = TRUE)
  
  data.frame(
    fpr = 1 - roc_obj$specificities,
    tpr = roc_obj$sensitivities,
    class = class,
    auc = rep(auc(roc_obj), length(roc_obj$sensitivities))
  )
}))

ggplot(roc_df, aes(x = fpr, y = tpr)) +
  geom_line(linewidth = 1.2, color = "steelblue") +
  geom_abline(linetype = "dashed", color = "gray") +
  facet_wrap(~ class, ncol = 3) +
  theme_minimal() +
  labs(
    title = "Test Set: One-vs-Rest ROC Curves for Each Class (Random Forest)",
    x = "False Positive Rate",
    y = "True Positive Rate"
  )

```

```{r}
# Main Results (Model Evaluation: Lasso)
# Step 1: Store predicted labels and true test labels
true.label = factor(test.data$type)
lasso.prediction = predict(model.results.pca[["Lasso"]], pca.test)

# Step 2: Create dataframe for evaluation
evaluate.data = data.frame(
  truth = true.label,
  prediction = factor(lasso.prediction, levels = levels(true.label))
)

# Step 3: F1
f1.macro = f_meas(evaluate.data, truth = truth, 
                  estimate = prediction, beta = 1)

# Step 4: F2
f2.macro = f_meas(evaluate.data, truth = truth, 
                  estimate = prediction, beta = 2)

# Step 5: Probabilities for AUC
lasso.probs = predict(model.results.pca[["Lasso"]], pca.test, type = "prob")
roc.multiclass = multiclass.roc(response = true.label, 
                                predictor = as.matrix(lasso.probs))
auc.value = auc(roc.multiclass)

# Step 6: Output
f1.macro
f2.macro
auc.value

# Step 7: ROC Curve Plot
class_levels = colnames(lasso.probs)

roc_df <- do.call(rbind, lapply(class_levels, function(class) {
  binary_response <- as.numeric(true.label == class)
    # Only compute ROC if both classes are present
  if (length(unique(binary_response)) < 2) {
    return(NULL)  # Skip this class
  }
  
  roc_obj <- roc(binary_response, lasso.probs[[class]], quiet = TRUE)
  
  data.frame(
    fpr = 1 - roc_obj$specificities,
    tpr = roc_obj$sensitivities,
    class = class,
    auc = rep(auc(roc_obj), length(roc_obj$sensitivities))
  )
}))

ggplot(roc_df, aes(x = fpr, y = tpr)) +
  geom_line(linewidth = 1.2, color = "steelblue") +
  geom_abline(linetype = "dashed", color = "gray") +
  facet_wrap(~ class, ncol = 3) +
  theme_minimal() +
  labs(
    title = "Test Set: One-vs-Rest ROC Curves for Each Class (Lasso)",
    x = "False Positive Rate",
    y = "True Positive Rate"
  )
```
```{r}
# Main Results (Model Evaluation: Decision Tree)
# Step 1: Store predicted labels and true test labels
true.label = factor(test.data$type)
tree.prediction = predict(model.results.pca[["Decision Tree"]], pca.test)

# Step 2: Create dataframe for evaluation
evaluate.data = data.frame(
  truth = true.label,
  prediction = factor(tree.prediction, levels = levels(true.label))
)

# Step 3: F1
f1.macro = f_meas(evaluate.data, truth = truth, 
                  estimate = prediction, beta = 1)

# Step 4: F2
f2.macro = f_meas(evaluate.data, truth = truth, 
                  estimate = prediction, beta = 2)

# Step 5: Probabilities for AUC
tree.probs = predict(model.results.pca[["Decision Tree"]], pca.test, type = "prob")
roc.multiclass = multiclass.roc(response = true.label, 
                                predictor = as.matrix(tree.probs))
auc.value = auc(roc.multiclass)

# Step 6: Output
f1.macro
f2.macro
auc.value

# Step 7: ROC Curve Plot
class_levels = colnames(tree.probs)

roc_df <- do.call(rbind, lapply(class_levels, function(class) {
  binary_response <- as.numeric(true.label == class)
    # Only compute ROC if both classes are present
  if (length(unique(binary_response)) < 2) {
    return(NULL)  # Skip this class
  }
  
  roc_obj <- roc(binary_response, tree.probs[[class]], quiet = TRUE)
  
  data.frame(
    fpr = 1 - roc_obj$specificities,
    tpr = roc_obj$sensitivities,
    class = class,
    auc = rep(auc(roc_obj), length(roc_obj$sensitivities))
  )
}))

ggplot(roc_df, aes(x = fpr, y = tpr)) +
  geom_line(linewidth = 1.2, color = "steelblue") +
  geom_abline(linetype = "dashed", color = "gray") +
  facet_wrap(~ class, ncol = 3) +
  theme_minimal() +
  labs(
    title = "Test Set: One-vs-Rest ROC Curves for Each Class (Decision Tree)",
    x = "False Positive Rate",
    y = "True Positive Rate"
  )
```

```{r}
# Main Results (Model Evaluation: KNN)
# Step 1: Store predicted labels and true test labels
true.label = factor(test.data$type)
knn.prediction = predict(model.results.pca[["KNN"]], pca.test)

# Step 2: Create dataframe for evaluation
evaluate.data = data.frame(
  truth = true.label,
  prediction = factor(knn.prediction, levels = levels(true.label))
)

# Step 3: F1
f1.macro = f_meas(evaluate.data, truth = truth, 
                  estimate = prediction, beta = 1)

# Step 4: F2
f2.macro = f_meas(evaluate.data, truth = truth, 
                  estimate = prediction, beta = 2)

# Step 5: Probabilities for AUC
knn.probs = predict(model.results.pca[["KNN"]], pca.test, type = "prob")
roc.multiclass = multiclass.roc(response = true.label, 
                                predictor = as.matrix(knn.probs))
auc.value = auc(roc.multiclass)

# Step 6: Output
f1.macro
f2.macro
auc.value

# Step 7: ROC Curve Plot
class_levels = colnames(knn.probs)

roc_df <- do.call(rbind, lapply(class_levels, function(class) {
  binary_response <- as.numeric(true.label == class)
    # Only compute ROC if both classes are present
  if (length(unique(binary_response)) < 2) {
    return(NULL)  # Skip this class
  }
  
  roc_obj <- roc(binary_response, knn.probs[[class]], quiet = TRUE)
  
  data.frame(
    fpr = 1 - roc_obj$specificities,
    tpr = roc_obj$sensitivities,
    class = class,
    auc = rep(auc(roc_obj), length(roc_obj$sensitivities))
  )
}))

ggplot(roc_df, aes(x = fpr, y = tpr)) +
  geom_line(linewidth = 1.2, color = "steelblue") +
  geom_abline(linetype = "dashed", color = "gray") +
  facet_wrap(~ class, ncol = 3) +
  theme_minimal() +
  labs(
    title = "Test Set: One-vs-Rest ROC Curves for Each Class (KNN)",
    x = "False Positive Rate",
    y = "True Positive Rate"
  )
```

